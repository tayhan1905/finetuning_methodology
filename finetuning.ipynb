{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee804b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original weight matrix (example)\n",
    "W = torch.randn(100, 200)  # Example shape [m=100, n=200]\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "\n",
    "# Top-k and bottom-r decomposition\n",
    "k = 50  # Choose the top-k singular values\n",
    "r = 50  # Choose the bottom-r singular values\n",
    "\n",
    "# Extract the top k singular values and vectors\n",
    "U_top = U[:, :k]\n",
    "S_top = torch.diag(S[:k])\n",
    "Vh_top = Vh[:k, :]\n",
    "\n",
    "# Extract the bottom r singular values and vectors\n",
    "U_bot = U[:, -r:]\n",
    "S_bot = torch.diag(S[-r:])\n",
    "Vh_bot = Vh[-r:, :]\n",
    "\n",
    "# Construct the two weight matrices\n",
    "W_top = U_top @ S_top @ Vh_top  # Top-k part\n",
    "W_bot = U_bot @ S_bot @ Vh_bot  # Bottom-r part\n",
    "\n",
    "# Verify that W is the sum of the two matrices\n",
    "W_reconstructed = W_top + W_bot\n",
    "print(torch.allclose(W, W_reconstructed))  # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9904a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = torch.linalg.svd(W_bot, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a612262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# SVD Decomposition Utility\n",
    "def compute_svd(W, r):\n",
    "    U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "    \n",
    "    # TOP R\n",
    "    U_top = U[:, :r]\n",
    "    S_top = S[:r]\n",
    "    Vh_top = Vh[:r, :]\n",
    "    \n",
    "    # BOTTOM R\n",
    "    U_bot = U[:, -r:]\n",
    "    S_bot = S[-r:]\n",
    "    Vh_bot = Vh[-r:, :]\n",
    "    \n",
    "    return U, S, Vh, U_top, S_top, Vh_top, U_bot, S_bot, Vh_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82070a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 8, 6\n",
    "W = torch.randn(m, n)\n",
    "\n",
    "# Choose r — require r <= min(m, n)\n",
    "r = 2\n",
    "p = min(m, n)\n",
    "assert r <= p, \"r must be <= min(m, n)\"\n",
    "# If you want top and bottom to be disjoint, also require 2r <= p\n",
    "\n",
    "# Call your helper\n",
    "U, S, Vh, U_top, S_top, Vh_top, U_bot, S_bot, Vh_bot = compute_svd(W, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4190dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "def truncated_svd(W, rank):\n",
    "    # thin SVD; rank <= min(out, in)\n",
    "    U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "    r = min(rank, S.numel())\n",
    "    return U[:, :r].contiguous(), S[:r].contiguous(), Vh[:r, :].contiguous()\n",
    "\n",
    "def svd_head_tail(W, r):\n",
    "    # return top-r and bottom-r blocks (disjoint if 2r <= p)\n",
    "    U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "    p = S.numel()\n",
    "    r_top = min(r, p)\n",
    "    r_bot = min(r, p - r_top) if (2*r <= p) else min(r, max(0, p - r_top))\n",
    "    U_top, S_top, Vh_top = U[:, :r_top], S[:r_top], Vh[:r_top, :]\n",
    "    U_bot, S_bot, Vh_bot = U[:, p-r_bot:], S[p-r_bot:], Vh[p-r_bot:, :]\n",
    "    return (U_top, S_top, Vh_top), (U_bot, S_bot, Vh_bot)\n",
    "\n",
    "def replace_module(parent, name, new_mod):\n",
    "    \"\"\"Set parent.<name> = new_mod\"\"\"\n",
    "    setattr(parent, name, new_mod)\n",
    "\n",
    "def iter_kv_linears(model):\n",
    "    \"\"\"\n",
    "    Yield (parent_module, attr_name, linear_module) for all K/V projections.\n",
    "    Matches common names: key/value, k_proj/v_proj\n",
    "    \"\"\"\n",
    "    for mod_name, mod in model.named_modules():\n",
    "        for attr in (\"key\", \"value\", \"k_proj\", \"v_proj\"):\n",
    "            if hasattr(mod, attr):\n",
    "                child = getattr(mod, attr)\n",
    "                if isinstance(child, nn.Linear):\n",
    "                    yield mod, attr, child\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a018bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaltEdoraLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Additive adapter around a frozen Linear:\n",
    "    y = x @ W^T + SALT_top(x) + eDoRA_tail(x)\n",
    "    - SALT: scale/shift top-r singulars (α, β)\n",
    "    - eDoRA: r x r core R in tail subspace\n",
    "    \"\"\"\n",
    "    def __init__(self, base_linear: nn.Linear, r: int, tail_mode='free'):\n",
    "        super().__init__()\n",
    "        assert isinstance(base_linear, nn.Linear)\n",
    "        self.base = base_linear\n",
    "        self.in_features  = base_linear.in_features\n",
    "        self.out_features = base_linear.out_features\n",
    "        self.bias = base_linear.bias is not None\n",
    "\n",
    "        ## Freezing the parameters from the pre trained model\n",
    "        # freeze the original weights that do not require training\n",
    "        self.base.weight.requires_grad_(False) \n",
    "        # freeze the original bias that do not need training as well\n",
    "        if self.bias:\n",
    "            self.base.bias.requires_grad_(False)\n",
    "\n",
    "        # SVD to provide some singular understanding of the pre trained matrix to guide our \n",
    "        W = self.base.weight.detach().to(torch.float32)\n",
    "        (U_top, S_top, Vh_top), (U_bot, S_bot, Vh_bot) = svd_head_tail(W, r)\n",
    "        dtype = self.base.weight.dtype\n",
    "        device = self.base.weight.device\n",
    "\n",
    "        # store basis as buffers >> saved as like self.U_top >>based on the string that is provided\n",
    "        self.register_buffer(\"U_top\", U_top.to(dtype).to(device))\n",
    "        self.register_buffer(\"S_top\", S_top.to(dtype).to(device))\n",
    "        self.register_buffer(\"Vh_top\", Vh_top.to(dtype).to(device))\n",
    "        self.register_buffer(\"U_bot\", U_bot.to(dtype).to(device))\n",
    "        self.register_buffer(\"S_bot\", S_bot.to(dtype).to(device))\n",
    "        self.register_buffer(\"Vh_bot\", Vh_bot.to(dtype).to(device))\n",
    "\n",
    "        # Understanding the total ranks of each of the weight matrices (top and tail)\n",
    "        self.r_top = S_top.numel()\n",
    "        self.r_bot = S_bot.numel()\n",
    "\n",
    "        # SALT params >> this is for the scale shift parameters\n",
    "        self.alpha = nn.Parameter(torch.zeros(self.r_top, dtype=dtype, device=device))\n",
    "        self.beta  = nn.Parameter(torch.zeros(self.r_top, dtype=dtype, device=device))\n",
    "\n",
    "        # eDoRA core >> free and polar\n",
    "        ## free >> the LORA style, no segregation of magnitude and directionality in this case\n",
    "        ## polar >> the DORA style, clear seegregation of magnitude and directionality\n",
    "        self.tail_mode = tail_mode\n",
    "        if self.r_bot > 0:\n",
    "            if tail_mode == 'free':\n",
    "                R0 = torch.zeros(self.r_bot, self.r_bot, dtype=dtype, device=device)\n",
    "                self.R = nn.Parameter(R0)  # neutral (no delta)\n",
    "            else:\n",
    "                raise ValueError(\"tail_mode must be 'free'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # base\n",
    "        y = F.linear(x, self.base.weight, self.base.bias) # y = x W^T + b\n",
    "\n",
    "        # SALT head delta\n",
    "        if self.r_top > 0:\n",
    "            zH = F.linear(x, self.Vh_top) # zH = x V_top^T\n",
    "\n",
    "            # Computation of the top singular values as scale shifts\n",
    "            delta_sigma = self.S_top * self.alpha + self.beta  # Δσ = S_top ⊙ α + β\n",
    "            y = y + F.linear(zH * delta_sigma, self.U_top) # y ← y + U_top diag(Δσ) V_top^T x\n",
    "\n",
    "        # eDoRA tail delta >> utilising the rxr matrix to run the update \n",
    "        if self.r_bot > 0:\n",
    "            zT = F.linear(x, self.Vh_bot) # zT = x V_bot^T                   \n",
    "            zR = F.linear(zT, self.R.T) # zR = zT R^T\n",
    "            y = y + F.linear(zR, self.U_bot)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec41add",
   "metadata": {},
   "source": [
    "## LORA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c329e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, base_linear: nn.Linear, r: int):\n",
    "        super().__init__()\n",
    "        self.base = base_linear\n",
    "        self.in_features = base_linear.in_features\n",
    "        self.out_features = base_linear.out_features\n",
    "\n",
    "        # Low-rank matrices for LoRA\n",
    "        self.A = nn.Parameter(torch.randn(self.in_features, r))  # Rank r\n",
    "        self.B = nn.Parameter(torch.randn(r, self.out_features))  # Rank r\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Base forward pass\n",
    "        return F.linear(x, self.base.weight, self.base.bias) + F.linear(x, self.A @ self.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e814d2",
   "metadata": {},
   "source": [
    "## DORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0182c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoRA(nn.Module):\n",
    "    def __init__(self, base_linear: nn.Linear, r: int):\n",
    "        super().__init__()\n",
    "        self.base = base_linear\n",
    "        self.in_features = base_linear.in_features\n",
    "        self.out_features = base_linear.out_features\n",
    "\n",
    "        # Magnitude parameter (scalar)\n",
    "        self.magnitude = nn.Parameter(torch.zeros(self.in_features))\n",
    "\n",
    "        # Directionality vector (unit vector)\n",
    "        self.direction = nn.Parameter(torch.randn(self.in_features, self.out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the directionality vector to have unit norm\n",
    "        direction_norm = F.normalize(self.direction, p=2, dim=0)\n",
    "        \n",
    "        # Magnitude scaling with directionality\n",
    "        delta_w = self.magnitude.unsqueeze(0) * direction_norm\n",
    "        return F.linear(x, self.base.weight, self.base.bias) + F.linear(x, delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf55ee",
   "metadata": {},
   "source": [
    "## SALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e007e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SALT(nn.Module):\n",
    "    def __init__(self, base_linear: nn.Linear, r: int):\n",
    "        super().__init__()\n",
    "        self.base = base_linear\n",
    "        self.in_features = base_linear.in_features\n",
    "        self.out_features = base_linear.out_features\n",
    "        \n",
    "        # Perform SVD on the original weight matrix (frozen)\n",
    "        W = self.base.weight.detach().to(torch.float32)\n",
    "        U, S, Vh = torch.svd(W)\n",
    "        \n",
    "        # Store SVD components as buffers\n",
    "        self.register_buffer(\"U\", U)\n",
    "        self.register_buffer(\"S\", S)\n",
    "        self.register_buffer(\"Vh\", Vh)\n",
    "        \n",
    "        # Define SALT parameters: alpha (scaling) and beta (shifting)\n",
    "        self.alpha = nn.Parameter(torch.zeros(r, dtype=torch.float32))\n",
    "        self.beta = nn.Parameter(torch.zeros(r, dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply scaling and shifting to the top singular values\n",
    "        delta_sigma = self.S[:self.alpha.size(0)] * self.alpha + self.beta\n",
    "        return F.linear(x, self.base.weight, self.base.bias) + F.linear(x, self.U[:, :self.alpha.size(0)] @ torch.diag(delta_sigma) @ self.Vh[:self.alpha.size(0), :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065402e4",
   "metadata": {},
   "source": [
    "## Replacement of linear layers with the new implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c55eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_qkv_with_adapter(model, r=8, mode=\"lora\"):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear) and (\"query\" in name or \"key\" in name or \"value\" in name):\n",
    "            if mode == \"lora\":\n",
    "                setattr(model, name, LoRA(module, r=r))\n",
    "            elif mode == \"dora\":\n",
    "                setattr(model, name, DoRA(module, r=r))\n",
    "            elif mode == \"saltedora\":\n",
    "                setattr(model, name, SaltEdoraLinear(module, r=r, tail_mode='free'))\n",
    "            elif mode == \"salt\":\n",
    "                setattr(model, name, SALT(module, r=r))\n",
    "        else:\n",
    "            replace_qkv_with_adapter(module, r=r, mode=mode)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e542ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tay Han\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# Load pretrained BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Replace Q, K, and V layers\n",
    "model = replace_qkv_with_adapter(base_model, 8, 'lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd19286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34528426785b4746877750e1c126fa7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eab720ff8f400d92f395bb0b3e4d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Load tokenizer for BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "tokenized.set_format(\"torch\")\n",
    "\n",
    "train_ds = tokenized[\"train\"]\n",
    "val_ds = tokenized[\"validation\"]\n",
    "\n",
    "# Evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d81997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "def train_and_evaluate(model, train_ds, val_ds):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    results = trainer.evaluate(val_ds)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Tay Han\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n",
      "Successful Implementation for LORA: query\n",
      "Successful Implementation for LORA: key\n",
      "Successful Implementation for LORA: value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1705e43398949f589db9240ab41759b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Experiment 1: LoRA\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_lora \u001b[38;5;241m=\u001b[39m replace_qkv_with_adapter(base_model, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m results_lora \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_lora\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoRA Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results_lora)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Experiment 2: DoRA\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_ds, val_ds)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate\u001b[39m(model, train_ds, val_ds):\n\u001b[0;32m     16\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     17\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     18\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     22\u001b[0m     )\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(val_ds)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2124\u001b[0m ):\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load base model (BERT)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Experiment 1: LoRA\n",
    "model_lora = replace_qkv_with_adapter(base_model, r=8, mode=\"lora\")\n",
    "results_lora = train_and_evaluate(model_lora, train_ds, val_ds)\n",
    "print(\"LoRA Results:\", results_lora)\n",
    "\n",
    "# Experiment 2: DoRA\n",
    "model_dora = replace_qkv_with_adapter(base_model, r=8, mode=\"dora\")\n",
    "results_dora = train_and_evaluate(model_dora, train_ds, val_ds)\n",
    "print(\"DoRA Results:\", results_dora)\n",
    "\n",
    "# Experiment 3: SaltEdoraLinear (Your custom implementation)\n",
    "model_saltedora = replace_qkv_with_adapter(base_model, r=8, mode=\"saltedora\")\n",
    "results_saltedora = train_and_evaluate(model_saltedora, train_ds, val_ds)\n",
    "print(\"SaltEdoraLinear Results:\", results_saltedora)\n",
    "\n",
    "# Experiment 4: SALT\n",
    "model_salt = replace_qkv_with_adapter(base_model, r=8, mode=\"salt\")\n",
    "results_salt = train_and_evaluate(model_salt, train_ds, val_ds)\n",
    "print(\"SALT Results:\", results_salt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bc12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
