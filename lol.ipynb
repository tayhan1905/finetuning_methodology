{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296636cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SALT Parameter Analysis\n",
      "Input dim: 768, Output dim: 768\n",
      "r_top: 32, tail_rank: 64\n",
      "--------------------------------------------------------------------------------\n",
      "Total parameters:     594,880\n",
      "Trainable parameters: 4,288\n",
      "Frozen parameters:    590,592\n",
      "Trainable %:          0.7208%\n",
      "--------------------------------------------------------------------------------\n",
      "Breakdown of trainable tensors:\n",
      "alpha                trainable          64\n",
      "beta                 trainable          64\n",
      "D                    trainable          64\n",
      "R                    trainable       4,096\n",
      "base.weight          frozen        589,824\n",
      "base.bias            frozen            768\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import SALT, SALTEdoraLinearV2 # or import directly if it's in the same file\n",
    "from utils.svd_utils import svd_head_tail\n",
    "\n",
    "def analyze_salt_trainables(in_features=768, out_features=768, r_top=32, tail_rank=64):\n",
    "    \"\"\"\n",
    "    Creates a dummy SALT layer with the given dimensions and prints:\n",
    "    - Total parameters\n",
    "    - Trainable parameters\n",
    "    - Non-trainable parameters\n",
    "    - Breakdown by tensor\n",
    "    \"\"\"\n",
    "    # Step 1️⃣: Create a dummy base linear layer (simulating BERT projection)\n",
    "    base = nn.Linear(in_features, out_features, bias=True)\n",
    "    salt = SALTEdoraLinearV2(base, tail_rank)\n",
    "\n",
    "    # Step 2️⃣: Count total & trainable parameters\n",
    "    total_params = sum(p.numel() for p in salt.parameters())\n",
    "    trainable_params = sum(p.numel() for p in salt.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"SALT Parameter Analysis\")\n",
    "    print(f\"Input dim: {in_features}, Output dim: {out_features}\")\n",
    "    print(f\"r_top: {r_top}, tail_rank: {tail_rank}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Frozen parameters:    {frozen_params:,}\")\n",
    "    print(f\"Trainable %:          {trainable_params / total_params * 100:.4f}%\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Step 3️⃣: Show detailed breakdown\n",
    "    print(\"Breakdown of trainable tensors:\")\n",
    "    for name, param in salt.named_parameters():\n",
    "        print(f\"{name:<20} {'trainable' if param.requires_grad else 'frozen':<10} {param.numel():>10,}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    return salt\n",
    "\n",
    "\n",
    "# Example usage (simulating BERT dimensions)\n",
    "if __name__ == \"__main__\":\n",
    "    # BERT attention projection (768 x 768)\n",
    "    salt_layer = analyze_salt_trainables(in_features=768, out_features=768, r_top=32, tail_rank=64)\n",
    "\n",
    "    # or BERT feedforward layer (3072 x 768)\n",
    "    # salt_layer = analyze_salt_trainables(in_features=768, out_features=3072, r_top=32, tail_rank=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a32d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_head_tail(W, r):\n",
    "    U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "    p = S.numel()\n",
    "\n",
    "    # Ensure we don't exceed the available rank\n",
    "    r_tail = min(r, p)\n",
    "    r_top = max(p - r_tail, 0)\n",
    "\n",
    "    # Split head/tail\n",
    "    U_top, S_top, Vh_top = U[:, :r_top], S[:r_top], Vh[:r_top, :]\n",
    "    U_tail, S_tail, Vh_tail = U[:, r_top:], S[r_top:], Vh[r_top:, :]\n",
    "\n",
    "    return (U_top, S_top, Vh_top), (U_tail, S_tail, Vh_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff5db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U_top: torch.Size([768, 764])\n",
      "S_top: torch.Size([764])\n",
      "Vh_top: torch.Size([764, 768])\n",
      "U_tail: torch.Size([768, 4])\n",
      "S_tail: torch.Size([4])\n",
      "Vh_tail: torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(768, 768)\n",
    "(head, tail) = svd_head_tail(W, r=4)\n",
    "\n",
    "print(\"U_top:\", head[0].shape)\n",
    "print(\"S_top:\", head[1].shape)\n",
    "print(\"Vh_top:\", head[2].shape)\n",
    "print(\"U_tail:\", tail[0].shape)\n",
    "print(\"S_tail:\", tail[1].shape)\n",
    "print(\"Vh_tail:\", tail[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4f59d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction relative error: 1.802e-06\n",
      "U_top: torch.Size([768, 764])\n",
      "S_top: torch.Size([764])\n",
      "Vh_top: torch.Size([764, 768])\n",
      "U_tail: torch.Size([768, 4])\n",
      "S_tail: torch.Size([4])\n",
      "Vh_tail: torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "W = torch.randn(768, 768)\n",
    "\n",
    "# Run your SVD split\n",
    "(head, tail) = svd_head_tail(W, r=4)\n",
    "\n",
    "# Unpack\n",
    "U_top, S_top, Vh_top = head\n",
    "U_tail, S_tail, Vh_tail = tail\n",
    "\n",
    "# ✅ Reconstruct W from head + tail\n",
    "W_head = (U_top * S_top) @ Vh_top\n",
    "W_tail = (U_tail * S_tail) @ Vh_tail\n",
    "W_recon = W_head + W_tail\n",
    "\n",
    "# ✅ Compute reconstruction error (should be near zero)\n",
    "recon_error = (W - W_recon).norm() / W.norm()\n",
    "print(f\"Reconstruction relative error: {recon_error.item():.3e}\")\n",
    "\n",
    "# ✅ Optional sanity check on shapes\n",
    "print(\"U_top:\", U_top.shape)\n",
    "print(\"S_top:\", S_top.shape)\n",
    "print(\"Vh_top:\", Vh_top.shape)\n",
    "print(\"U_tail:\", U_tail.shape)\n",
    "print(\"S_tail:\", S_tail.shape)\n",
    "print(\"Vh_tail:\", Vh_tail.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5871867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3186, -0.7997,  0.6924,  ..., -0.0438, -1.4127, -0.9437],\n",
       "        [-0.9846, -2.0882,  1.5418,  ...,  1.9014,  0.2866, -1.0197],\n",
       "        [ 0.5301, -1.9153, -1.0040,  ...,  0.0254, -1.7016,  0.8479],\n",
       "        ...,\n",
       "        [-0.4989, -0.2758,  1.0541,  ...,  0.2765, -0.4789, -0.6002],\n",
       "        [-2.7582, -1.0077,  0.9167,  ..., -1.2549,  1.7061, -0.6058],\n",
       "        [ 0.3568,  1.0276,  1.3051,  ...,  1.0987, -0.3125, -1.8925]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0920eb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3186, -0.7997,  0.6924,  ..., -0.0438, -1.4127, -0.9437],\n",
       "        [-0.9847, -2.0882,  1.5418,  ...,  1.9014,  0.2866, -1.0197],\n",
       "        [ 0.5301, -1.9153, -1.0040,  ...,  0.0254, -1.7016,  0.8479],\n",
       "        ...,\n",
       "        [-0.4989, -0.2758,  1.0541,  ...,  0.2765, -0.4789, -0.6002],\n",
       "        [-2.7582, -1.0077,  0.9167,  ..., -1.2549,  1.7061, -0.6058],\n",
       "        [ 0.3568,  1.0276,  1.3051,  ...,  1.0987, -0.3125, -1.8925]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_head + W_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3133ffad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7981e-04,  5.9225e-04, -2.0136e-04,  ..., -1.5246e-04,\n",
       "          8.7929e-05,  1.6057e-04],\n",
       "        [-2.9198e-04,  2.6308e-04, -1.9486e-04,  ...,  6.7842e-06,\n",
       "          1.1114e-04, -2.4365e-05],\n",
       "        [-1.2387e-04,  1.6415e-04, -7.1647e-05,  ..., -3.0214e-05,\n",
       "          6.3690e-05, -9.4080e-06],\n",
       "        ...,\n",
       "        [ 3.0820e-04, -1.6131e-04,  2.0097e-04,  ..., -4.1044e-06,\n",
       "          8.5188e-05, -1.2484e-04],\n",
       "        [-2.5827e-04,  4.5846e-04, -9.9390e-05,  ..., -1.3269e-04,\n",
       "          1.7327e-04,  7.1233e-06],\n",
       "        [ 1.1637e-04, -2.7852e-05,  9.4625e-05,  ..., -5.1087e-05,\n",
       "         -1.9922e-05,  1.1857e-05]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_tail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
